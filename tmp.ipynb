{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8de3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuvin\\OneDrive\\Desktop\\OmniGen\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "class DatasetFromJson(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_file: str, \n",
    "        tokenizer: str,\n",
    "        vae_tokenizer: str,\n",
    "        max_input_length_limit: int = 18000,\n",
    "    ):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.vae_tokenizer = vae_tokenizer\n",
    "        self.max_input_length_limit = max_input_length_limit\n",
    "\n",
    "        self.data = load_dataset('json', data_files=json_file)['train']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        \n",
    "        input_, output = example['input'], example['output']\n",
    "        input_ = f\"{input_}\\n<bot>\"\n",
    "        output = f\"{output}{self.tokenizer.eos_token}\"\n",
    "\n",
    "        tokenized_input = self.tokenizer(input_)\n",
    "        tokenized_output = self.vae_tokenizer(output)\n",
    "\n",
    "        return {\n",
    "            \"input\": tokenized_input,\n",
    "            \"output\": tokenized_output,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cd8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class OmniGenCollator:\n",
    "    def __init__(self, tokenizer, hidden_size=3072):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        batch: list of {\"input\": BatchEncoding, \"output\": BatchEncoding}\n",
    "        Returns dict with keys:\n",
    "        - input_ids, attention_mask        (for the encoder/prompt)\n",
    "        - labels                            (for the decoder/LM loss)\n",
    "        \"\"\"\n",
    "        # 1. Separate inputs & outputs\n",
    "        inputs  = [example[\"input\"]  for example in batch]\n",
    "        outputs = [example[\"output\"] for example in batch]\n",
    "\n",
    "        # 2. Pad both to the longest sequence in this batch\n",
    "        batch_inputs  = self.tokenizer.pad(inputs, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        # 3. Create position ids.\n",
    "        attn = batch_inputs[\"attention_mask\"]\n",
    "        position_ids = torch.cumsum(attn, dim=1) * attn\n",
    "\n",
    "        B, N = position_ids.size()\n",
    "        max_output_len = max(len(o[\"input_ids\"]) for o in outputs)\n",
    "        extra_cols    = max_output_len + 1\n",
    "\n",
    "        lengths = attn.sum(dim=1)\n",
    "\n",
    "        base = torch.arange(1, extra_cols+1, device=position_ids.device)\n",
    "        pad  = base.unsqueeze(0).expand(B, -1) + lengths.unsqueeze(1)\n",
    "\n",
    "        position_ids = torch.cat([position_ids, pad], dim=1)\n",
    "\n",
    "        # 4. Create attention mask.\n",
    "        temp_l = torch.sum(batch_inputs['attention_mask'], dim=-1)\n",
    "\n",
    "        text_length = batch_inputs['attention_mask'].size(-1)\n",
    "        seq_len = text_length + max_output_len + 1\n",
    "\n",
    "        attn_masks = []\n",
    "        for idx, i in enumerate(temp_l):\n",
    "            temp_mask = torch.tril(torch.ones(size=(i+1, i+1)))\n",
    "\n",
    "            image_mask = torch.zeros(size=(i+1, max_output_len))\n",
    "            temp_mask = torch.cat([temp_mask, image_mask], dim=-1)\n",
    "\n",
    "            image_mask = torch.ones(size=(max_output_len, i+max_output_len+1))\n",
    "            temp_mask = torch.cat([temp_mask, image_mask], dim=0)\n",
    "\n",
    "            pad_l = text_length - i\n",
    "            if pad_l > 0:\n",
    "                pad_mask = torch.zeros(size=(i+1+max_output_len, pad_l))\n",
    "                temp_mask = torch.cat([pad_mask, temp_mask], dim=-1)\n",
    "\n",
    "                pad_mask = torch.ones(size=(pad_l, seq_len))\n",
    "                temp_mask = torch.cat([pad_mask, temp_mask], dim=0)\n",
    "\n",
    "            true_img_length = len(outputs[idx]['input_ids'])\n",
    "            pad_img_length = max_output_len - true_img_length\n",
    "            if pad_img_length > 0:\n",
    "                temp_mask[:, -pad_img_length:] = 0\n",
    "\n",
    "            attn_masks.append(temp_mask.unsqueeze(0))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": batch_inputs.input_ids,\n",
    "            \"attention_mask\": torch.cat(attn_masks, dim=0),\n",
    "            \"position_ids\": position_ids,\n",
    "            \"output\": [torch.tensor(i.input_ids).unsqueeze(0) for i in outputs],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0b1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def vae_encode_list_new(model, output):\n",
    "    _ = model.eval()\n",
    "\n",
    "    output_latents = []\n",
    "    for i in output:\n",
    "        with torch.no_grad():\n",
    "            o = model(input_ids=i, output_hidden_states=True,)\n",
    "        o = o.hidden_states[-1]\n",
    "        output_latents.append(o)\n",
    "\n",
    "    return output_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac46b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds scalar timesteps into vector representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
    "        )\n",
    "        self.frequency_embedding_size = frequency_embedding_size\n",
    "\n",
    "        # Initialize timestep embedding MLP:\n",
    "        nn.init.normal_(self.mlp[0].weight, std=0.02)\n",
    "        nn.init.normal_(self.mlp[2].weight, std=0.02)\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(t, dim, max_period=10000):\n",
    "        \"\"\"\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
    "                          These may be fractional.\n",
    "        :param dim: the dimension of the output.\n",
    "        :param max_period: controls the minimum frequency of the embeddings.\n",
    "        :return: an (N, D) Tensor of positional embeddings.\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, t, dtype=torch.float32):\n",
    "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size).to(dtype)\n",
    "        print(t_freq.shape)\n",
    "        t_emb = self.mlp(t_freq)\n",
    "        return t_emb\n",
    "    \n",
    "def modulate(x, shift, scale):\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "\n",
    "class TextFinalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The “final layer” for a text‐only OmniGen: normalize, apply AdaLN\n",
    "    from the timestep embedding, then project to vocab logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "        # 1) Final norm (no affine because we modulate instead)\n",
    "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        # 2) Modulation MLP: turn timestep embedding -> [shift | scale]\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n",
    "        )\n",
    "        # 3) LM head\n",
    "        self.lm_head = nn.Linear(hidden_size, output_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, t_emb: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "        x     : (B, L, H)  — hidden states from your transformer\n",
    "        t_emb : (B, H)     — timestep (or other) conditioning embedding\n",
    "        returns logits: (B, L, output_size)\n",
    "        \"\"\"\n",
    "        # compute shift & scale\n",
    "        shift, scale = self.adaLN_modulation(t_emb).chunk(2, dim=1)  # each (B, H)\n",
    "\n",
    "        # normalize + apply FiLM-style modulation\n",
    "        x = modulate(self.norm_final(x), shift, scale)              # (B, L, H)\n",
    "\n",
    "        # project to vocabulary\n",
    "        logits = self.lm_head(x)                                    # (B, L, V)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49269d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "from transformers import Phi3Config, Phi3Model\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class Phi3Transformer(Phi3Model):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Phi3DecoderLayer`]\n",
    "    We only modified the attention mask\n",
    "    Args:\n",
    "        config: Phi3Config\n",
    "    \"\"\"\n",
    "    def prefetch_layer(self, layer_idx: int, device: torch.device):\n",
    "        \"Starts prefetching the next layer cache\"\n",
    "        with torch.cuda.stream(self.prefetch_stream):\n",
    "            # Prefetch next layer tensors to GPU\n",
    "            for name, param in self.layers[layer_idx].named_parameters():\n",
    "                param.data = param.data.to(device, non_blocking=True)\n",
    "\n",
    "    def evict_previous_layer(self, layer_idx: int):\n",
    "        \"Moves the previous layer cache to the CPU\"\n",
    "        prev_layer_idx = layer_idx - 1\n",
    "        for name, param in self.layers[prev_layer_idx].named_parameters():\n",
    "            param.data = param.data.to(\"cpu\", non_blocking=True)\n",
    "            \n",
    "    def get_offlaod_layer(self, layer_idx: int, device: torch.device):\n",
    "        # init stream\n",
    "        if not hasattr(self, \"prefetch_stream\"):\n",
    "            self.prefetch_stream = torch.cuda.Stream()\n",
    "\n",
    "        # delete previous layer\n",
    "        torch.cuda.current_stream().synchronize()\n",
    "        self.evict_previous_layer(layer_idx)\n",
    "        \n",
    "        # make sure the current layer is ready\n",
    "        torch.cuda.synchronize(self.prefetch_stream)\n",
    "\n",
    "        # load next layer\n",
    "        self.prefetch_layer((layer_idx + 1) % len(self.layers), device)\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        offload_model: Optional[bool] = False,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "        return_legacy_cache = False\n",
    "        if use_cache and not isinstance(past_key_values, Cache):\n",
    "            return_legacy_cache = True\n",
    "            if past_key_values is None:\n",
    "                past_key_values = DynamicCache()\n",
    "            else:\n",
    "                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "                logger.warning_once(\n",
    "                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n",
    "                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n",
    "                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n",
    "                )\n",
    "\n",
    "        # if inputs_embeds is None:\n",
    "        #     inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        # if cache_position is None:\n",
    "        #     past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "        #     cache_position = torch.arange(\n",
    "        #         past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "        #     )\n",
    "        # if position_ids is None:\n",
    "        #     position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        if attention_mask is not None and attention_mask.dim() == 3:\n",
    "            dtype = inputs_embeds.dtype\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "            attention_mask = (1 - attention_mask) * min_dtype\n",
    "            attention_mask = attention_mask.unsqueeze(1).to(inputs_embeds.dtype)\n",
    "        else:\n",
    "            raise Exception(\"attention_mask parameter was unavailable or invalid\")\n",
    "            # causal_mask = self._update_causal_mask(\n",
    "            #     attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "            # )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        layer_idx = -1\n",
    "        for decoder_layer in self.layers:\n",
    "            layer_idx += 1\n",
    "\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                )\n",
    "            else:\n",
    "                if offload_model and not self.training:\n",
    "                    self.get_offlaod_layer(layer_idx, device=inputs_embeds.device)\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6926dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import Phi3Config\n",
    "\n",
    "class OmniGen(nn.Module):\n",
    "    def __init__(self, new_embed_size, hidden_size=3072):\n",
    "        super().__init__()\n",
    "\n",
    "        config = Phi3Config.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "        self.llm = Phi3Transformer(config)\n",
    "        self.llm.resize_token_embeddings(new_embed_size)\n",
    "\n",
    "        self.time_token = TimestepEmbedder(hidden_size)\n",
    "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
    "\n",
    "        self.final_layer = TextFinalLayer(hidden_size=hidden_size, output_size=hidden_size)\n",
    "\n",
    "    def pad_x(self, x):\n",
    "        max_len = max(t.size(1) for t in x)\n",
    "        num_tokens = [t.size(1) for t in x]\n",
    "        padded_latents = []\n",
    "        for t in x:\n",
    "            L = t.size(1)\n",
    "            pad_len = max_len - L\n",
    "            if pad_len > 0:\n",
    "                t = F.pad(t, (0, 0, 0, pad_len), mode=\"constant\", value=0)\n",
    "            padded_latents.append(t)\n",
    "\n",
    "        batch = torch.cat(padded_latents, dim=0)\n",
    "\n",
    "        return batch, num_tokens\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x,\n",
    "        timestep,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        position_ids,\n",
    "        past_key_values=None,\n",
    "        offload_model:bool=False\n",
    "    ):\n",
    "        x_padded, num_tokens = self.pad_x(x)\n",
    "        time_token = self.time_token(timestep, dtype=x[0].dtype).unsqueeze(1)   \n",
    "        condition_embeds = self.llm.embed_tokens(input_ids).clone()\n",
    "\n",
    "        input_emb = torch.cat([condition_embeds, time_token, x_padded], dim=1)\n",
    "\n",
    "        output = self.llm(\n",
    "            inputs_embeds=input_emb, \n",
    "            attention_mask=attention_mask, \n",
    "            position_ids=position_ids, \n",
    "            past_key_values=past_key_values, \n",
    "            offload_model=offload_model\n",
    "        )\n",
    "\n",
    "        out_embedding = output.last_hidden_state[:, -max(num_tokens):]\n",
    "        time_emb = self.t_embedder(timestep, dtype=x[0].dtype)\n",
    "        out_x = self.final_layer(out_embedding, time_emb)\n",
    "\n",
    "        latents = []\n",
    "        for i in range(out_x.size(0)):\n",
    "            latent = out_x[i:i+1, :num_tokens[i]]\n",
    "            latents.append(latent)\n",
    "\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_x0(x1):\n",
    "    \"\"\"Sampling x0 & t based on shape of x1 (if needed)\n",
    "    Args:\n",
    "      x1 - data point; [batch, *dim]\n",
    "    \"\"\"\n",
    "    if isinstance(x1, (list, tuple)):\n",
    "        x0 = [torch.randn_like(img_start) for img_start in x1]\n",
    "    else:\n",
    "        x0 = torch.randn_like(x1)\n",
    "\n",
    "    return x0\n",
    "\n",
    "def sample_timestep(x1):\n",
    "    u = torch.normal(mean=0.0, std=1.0, size=(len(x1),))\n",
    "    t = 1 / (1 + torch.exp(-u))\n",
    "    t = t.to(x1[0])\n",
    "    return t\n",
    "\n",
    "def training_losses(model, x1, model_kwargs=None):\n",
    "    \"\"\"Loss for training torche score model\n",
    "    Args:\n",
    "    - model: backbone model; could be score, noise, or velocity\n",
    "    - x1: datapoint\n",
    "    - model_kwargs: additional arguments for torch model\n",
    "    \"\"\"\n",
    "    B = len(x1)\n",
    "\n",
    "    x0 = sample_x0(x1)\n",
    "    t = sample_timestep(x1)\n",
    "\n",
    "    xt = [t[i] * x1[i] + (1 - t[i]) * x0[i] for i in range(B)]\n",
    "    ut = [x1[i] - x0[i] for i in range(B)]\n",
    "\n",
    "    model_output = model(xt, t, **model_kwargs)  # (B, C, H, W) -> (B, L, D)\n",
    "\n",
    "    loss_per_sample = torch.stack([\n",
    "        ((ut[i] - model_output[i])**2).mean()\n",
    "        for i in range(len(model_output))\n",
    "    ], dim=0)\n",
    "    loss = loss_per_sample.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697192d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"<bot>\"]}\n",
    "num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "vae_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "\n",
    "dataset = DatasetFromJson(json_file=\"cd4_train.jsonl\", tokenizer=tokenizer, vae_tokenizer=vae_tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=OmniGenCollator(tokenizer=tokenizer),   # <-- all padding happens here\n",
    "    num_workers=0,\n",
    ")\n",
    "data = next(iter(dataloader))\n",
    "\n",
    "model = OmniGen(\n",
    "    new_embed_size=len(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c040a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: linear layer to map vae output to D=3072\n",
    "# TODO: add <cfg> for condition dropout \n",
    "# TODO: use our VAE\n",
    "# TODO: train on hf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "746a761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.33s/it]\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "vae = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "\n",
    "output_encoded = vae_encode_list_new(\n",
    "    vae,  # insert own model here\n",
    "    data['output']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256])\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "loss = training_losses(\n",
    "    model, \n",
    "    output_encoded, \n",
    "    model_kwargs={\n",
    "        \"input_ids\": data['input_ids'],\n",
    "        \"attention_mask\": data['attention_mask'],\n",
    "        \"position_ids\": data['position_ids'],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "625b9f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 3072]) torch.Size([1, 27, 3072]) torch.Size([1, 27, 3072])\n",
      "torch.Size([1, 28, 3072]) torch.Size([1, 28, 3072]) torch.Size([1, 28, 3072])\n",
      "torch.Size([1, 32, 3072]) torch.Size([1, 32, 3072]) torch.Size([1, 32, 3072])\n",
      "torch.Size([1, 25, 3072]) torch.Size([1, 25, 3072]) torch.Size([1, 25, 3072])\n",
      "torch.Size([1, 25, 3072]) torch.Size([1, 25, 3072]) torch.Size([1, 25, 3072])\n",
      "torch.Size([1, 31, 3072]) torch.Size([1, 31, 3072]) torch.Size([1, 31, 3072])\n",
      "torch.Size([1, 24, 3072]) torch.Size([1, 24, 3072]) torch.Size([1, 24, 3072])\n",
      "torch.Size([1, 26, 3072]) torch.Size([1, 26, 3072]) torch.Size([1, 26, 3072])\n",
      "torch.Size([1, 27, 3072]) torch.Size([1, 27, 3072]) torch.Size([1, 27, 3072])\n",
      "torch.Size([1, 30, 3072]) torch.Size([1, 30, 3072]) torch.Size([1, 30, 3072])\n",
      "torch.Size([1, 32, 3072]) torch.Size([1, 32, 3072]) torch.Size([1, 32, 3072])\n",
      "torch.Size([1, 27, 3072]) torch.Size([1, 27, 3072]) torch.Size([1, 27, 3072])\n",
      "torch.Size([1, 28, 3072]) torch.Size([1, 28, 3072]) torch.Size([1, 28, 3072])\n",
      "torch.Size([1, 26, 3072]) torch.Size([1, 26, 3072]) torch.Size([1, 26, 3072])\n",
      "torch.Size([1, 26, 3072]) torch.Size([1, 26, 3072]) torch.Size([1, 26, 3072])\n",
      "torch.Size([1, 26, 3072]) torch.Size([1, 26, 3072]) torch.Size([1, 26, 3072])\n"
     ]
    }
   ],
   "source": [
    "for i, j, k in zip(model_output, xt, ut):\n",
    "    print(i.shape, j.shape, k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fd285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import get_scheduler\n",
    "import math\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "lr = 1e-4\n",
    "lr_scheduler = \"constant\"\n",
    "lr_warmup_steps = 1000\n",
    "gradient_accumulation_steps = 1\n",
    "adam_weight_decay = 0.0\n",
    "report_to = \"wandb\"\n",
    "results_dir = \"results\"\n",
    "mixed_precision = \"bf16\"\n",
    "\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=adam_weight_decay)\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(dataloader) / gradient_accumulation_steps)\n",
    "max_train_steps = epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    lr_scheduler,\n",
    "    optimizer=opt,\n",
    "    num_warmup_steps=lr_warmup_steps * gradient_accumulation_steps,\n",
    "    num_training_steps=max_train_steps * gradient_accumulation_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "_ = model.train()\n",
    "\n",
    "train_steps = 0\n",
    "running_loss = 0\n",
    "start_time = time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            output_encoded = vae_encode_list_new(\n",
    "                vae,  # insert own model here\n",
    "                data['output']\n",
    "            )\n",
    "\n",
    "        loss = training_losses(\n",
    "            model, \n",
    "            output_encoded, \n",
    "            model_kwargs={\n",
    "                \"input_ids\": data['input_ids'],\n",
    "                \"attention_mask\": data['attention_mask'],\n",
    "                \"position_ids\": data['position_ids'],\n",
    "            }\n",
    "        )\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # 4) optimizer step when we've accumulated enough\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "            opt.zero_grad()\n",
    "            train_steps += 1\n",
    "            running_loss += loss.item() * gradient_accumulation_steps  # un-normalize\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
